import logging

import ray
import nltk
import evaluate as hf_evaluate

from dataclasses import dataclass
from typing import Optional, List, Callable, Dict, Any
from ray.data import Dataset
from nltk import word_tokenize
from nltk.translate import meteor_score

import fmeval.util as util
from fmeval.constants import (
    DatasetColumns,
    MEAN,
)
from fmeval.data_loaders.util import DataConfig, get_dataset
from fmeval.eval_algorithms import (
    EvalAlgorithm,
    EvalScore,
    EvalOutput,
    EVAL_DATASETS,
    DATASET_CONFIGS,
    get_default_prompt_template,
)
from fmeval.eval_algorithms.eval_algorithm import EvalAlgorithmInterface, EvalAlgorithmConfig
from fmeval.eval_algorithms.helper_models.helper_model import BertscoreHelperModel
from fmeval.eval_algorithms.util import (
    generate_prompt_column_for_dataset,
    generate_model_predict_response_for_dataset,
    validate_dataset,
    aggregate_evaluation_scores,
    generate_output_dataset_path,
    save_dataset,
)
from fmeval.exceptions import EvalAlgorithmClientError
from fmeval.model_runners.model_runner import ModelRunner
from fmeval.perf_util import timed_block
from fmeval.transforms.summarization_accuracy import SummarizationAccuracyTransforms
from fmeval.transforms.transform import Record
from fmeval.transforms.transform_pipeline import TransformPipeline
from fmeval.transforms.util import GeneratePrompt, GetModelResponse, shared_resource

METEOR_SCORE = "meteor"
ROUGE_SCORE = "rouge"
BERT_SCORE = "bertscore"
METRIC_NAMES = [METEOR_SCORE, ROUGE_SCORE, BERT_SCORE]

# rouge constants
ROUGE_1 = "rouge1"
ROUGE_2 = "rouge2"
ROUGE_L = "rougeL"

ROUGE_TYPES = [ROUGE_1, ROUGE_2, ROUGE_L]

# bertscore constants
MICROSOFT_DEBERTA_MODEL = "microsoft/deberta-xlarge-mnli"
ROBERTA_MODEL = "roberta-large-mnli"
DEFAULT_MODEL_TYPE = MICROSOFT_DEBERTA_MODEL
MODEL_TYPES_SUPPORTED = [MICROSOFT_DEBERTA_MODEL, ROBERTA_MODEL]

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class SummarizationAccuracyConfig(EvalAlgorithmConfig):
    """
    Configuration for the summarization accuracy eval algorithm

    :param rouge_type: Type of rouge metric in eval results
    :param use_stemmer_for_rouge: bool value to set using stemmer for rouge metric
    :param bertscore_model_type: model to use for bert score
    """

    rouge_type: str = ROUGE_2
    use_stemmer_for_rouge: bool = True
    bertscore_model_type: str = DEFAULT_MODEL_TYPE

    def __post_init__(self):
        if self.rouge_type not in ROUGE_TYPES:
            raise EvalAlgorithmClientError(
                f"Invalid rouge_type: {self.rouge_type} requested in SummarizationAccuracyConfig, "
                f"please choose from acceptable values: {ROUGE_TYPES}"
            )

        if self.bertscore_model_type not in MODEL_TYPES_SUPPORTED:
            raise EvalAlgorithmClientError(
                f"Invalid bertscore_model_type: {self.bertscore_model_type} requested in "
                f"SummarizationAccuracyConfig, please choose from acceptable values: {MODEL_TYPES_SUPPORTED}"
            )


class SummarizationAccuracy(EvalAlgorithmInterface):
    """
    Summarization Accuracy Eval algorithm

    The aim of this eval algo is to evaluate how well a model can summarize text.
    The algo uses a reference summary to compare the output generated by the model and a series
    of quality metrics based on overlapping between words (ROUGE and METEOR) and similarity scores (BERTScore).
    """

    def __init__(self, eval_algorithm_config: SummarizationAccuracyConfig = SummarizationAccuracyConfig()):
        super().__init__(eval_algorithm_config)
        self.eval_name = EvalAlgorithm.SUMMARIZATION_ACCURACY.value
        bertscore_model = shared_resource(BertscoreHelperModel(eval_algorithm_config.bertscore_model_type))
        summ_acc = SummarizationAccuracyTransforms(
            target_output_key=DatasetColumns.TARGET_OUTPUT.value.name,
            model_output_key=DatasetColumns.MODEL_OUTPUT.value.name,
            bertscore_model=bertscore_model,
            rouge_type=eval_algorithm_config.rouge_type,
            use_stemmer_for_rouge=eval_algorithm_config.use_stemmer_for_rouge,
        )
        self.pipeline = summ_acc.pipeline

    @staticmethod
    def create_sample(target_output: str, model_output: str) -> Record:
        return {
            DatasetColumns.TARGET_OUTPUT.value.name: target_output,
            DatasetColumns.MODEL_OUTPUT.value.name: model_output
        }

    def evaluate_sample(self, target_output: str, model_output: str) -> List[EvalScore]:  # type: ignore[override]
        sample = SummarizationAccuracy.create_sample(target_output=target_output, model_output=model_output)
        output_record = self.pipeline.execute([sample])[0]
        assert all(metric_name in output_record for metric_name in METRIC_NAMES)
        return [
            EvalScore(name=metric_name, value=output_record[metric_name])
            for metric_name in METRIC_NAMES
        ]

    def evaluate(
            self,
            model: Optional[ModelRunner] = None,
            dataset_config: Optional[DataConfig] = None,
            prompt_template: Optional[str] = None,
            save: bool = False,
            num_records=100,
    ) -> List[EvalOutput]:
        """
        Summarization Accuracy evaluate

        :param model: An instance of ModelRunner which is the model under evaluation
        :param dataset_config: Configures the single dataset used for evaluation. If not provided,
            evaluation will use all of it's supported built-in datasets
        :param prompt_template: A template which can be used to generate prompts, optional, if not provided defaults
            will be used.
        :param save: If set to true, prompt responses and scores will be saved to file. The output is written to
                     EvalAlgorithmInterface.EVAL_RESULTS_PATH
        :param num_records: The number of records to be sampled randomly from the input dataset to perform the
                            evaluation

        :return: List of EvalOutput objects.
        """
        if dataset_config:
            dataset_configs = [dataset_config]
        else:
            dataset_configs = [DATASET_CONFIGS[dataset_name] for dataset_name in EVAL_DATASETS[self.eval_name]]

        eval_outputs = []
        for dataset_config in dataset_configs:
            dataset = get_dataset(dataset_config, num_records)
            validate_dataset(dataset, [DatasetColumns.TARGET_OUTPUT.value.name, DatasetColumns.MODEL_INPUT.value.name])
            dataset_prompt_template = None
            pipeline = self.pipeline

            if DatasetColumns.MODEL_OUTPUT.value.name not in dataset.columns():
                util.require(model, "No ModelRunner provided. ModelRunner is required for inference on model_inputs")
                dataset_prompt_template = (
                    get_default_prompt_template(dataset_config.dataset_name) if not prompt_template else prompt_template
                )
                gen_prompt = GeneratePrompt(
                    input_keys=[DatasetColumns.MODEL_INPUT.value.name],
                    output_keys=[DatasetColumns.PROMPT.value.name],
                    prompt_template=prompt_template,
                )
                get_model_response = GetModelResponse(
                    input_keys=gen_prompt.output_keys,
                    output_keys=[DatasetColumns.MODEL_OUTPUT.value.name],
                    model_runner=model,
                )
                pipeline = TransformPipeline([gen_prompt, get_model_response], pipeline)

            with timed_block(f"Computing score and aggregation on dataset {dataset_config.dataset_name}", logger):
                dataset = pipeline.execute(dataset)
                dataset_scores, category_scores = aggregate_evaluation_scores(
                    dataset, [METEOR_SCORE, ROUGE_SCORE, BERT_SCORE], agg_method=MEAN
                )
                eval_outputs.append(
                    EvalOutput(
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                        prompt_template=dataset_prompt_template,
                        dataset_scores=dataset_scores,
                        category_scores=category_scores,
                        output_path=generate_output_dataset_path(
                            path_to_parent_dir=self._eval_results_path,
                            eval_name=self.eval_name,
                            dataset_name=dataset_config.dataset_name,
                        ),
                    )
                )
            if save:
                save_dataset(
                    dataset=dataset,
                    score_names=[METEOR_SCORE, ROUGE_SCORE, BERT_SCORE],
                    path=generate_output_dataset_path(
                        path_to_parent_dir=self._eval_results_path,
                        eval_name=self.eval_name,
                        dataset_name=dataset_config.dataset_name,
                    ),
                )

        return eval_outputs
